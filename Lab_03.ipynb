{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "3db34a80-4460-4b2c-b6d3-499ae7cc2a7d",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 66
   },
   "source": "# Start writing code here...",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Lab 03 - Spectral Indices & Transformations\n\n## Overview\n\nIn this lab, we will work with the spectral characteristics in our data to visualize and extract insights that go beyond basic visual interpretation. We will work with the different spectral bands offered by Landsat 8 to find unique patterns that can help us solve problems and conduct analysis. By the end of this lab, you should be able to understand how to build and visualize existing indices, as well as construct your own, identify how different indices can help your use case, and understand the mechanism behind how they work. \n\n## Spectral Indices\n\nSpectroscopy is the study of how radiation is absorbed, reflected and emitted by different materials. While this discipline has its origins in chemistry and physics, we can utilize the same techniques to identify different land cover types from satellite data.  In the chart below, land cover types have unique spectral characteristics. Snow has a major peak at lower wavelengths and is near zero above 1.5 micrometer, whereas soil has very low reflectance at lower levels of wavelength but relatively strong and steady reflectance after ~0.75 micrometers. Spectral indices are built to leverage these unique characteristics and isolate specific types of land cover. \n\nLand covers are separable at different wavelengths. Vegetation curves (green) have high reflectance in the NIR range, where radiant energy is scattered by cell walls ([Bowker, 1985](http://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/19850022138.pdf)) and low reflectance in the red range, where radiant energy is [absorbed by chlorophyll](https://en.wikipedia.org/wiki/Chlorophyll#/media/File:Chlorophyll_ab_spectra-en.svg). We can leverage this information to build indices that help us differentiate vegetation from urban areas. In the next few sections, we will cover several of the most important indices in use. \n\n![Land Cover Reflectance](im/im_03_01.png)\n\n## Important Indices \n\n#### Normalized Difference Vegetation Index (NDVI)\n\nThe Normalized Difference Vegetation Index (NDVI) has a [long history](https://en.wikipedia.org/wiki/Normalized_Difference_Vegetation_Index) in remote sensing, and is one of the most widely used measures. The typical formulation is:\n\n$$\\text{NDVI} = (\\text{NIR} - \\text{red}) / (\\text{NIR} + \\text{red})$$\n\nWhere *NIR* refers to the near infrared band and *red* refers to the red peak in the visible spectrum.\n\nBecause NDVI is a popular and well-known index, we can use the built-in functionality within Earth Engine `normalizedDifference()`to calculate NDVI. You can follow the steps below to build your own indices. \n\nFirst, build a baseline true color image around our region of interest, Blacksburg, VA. We will work with the Landsat 8 Collection 1 Tier 1 TOA Reflectance data from 2015, sort by cloud cover and extract the first image.\n\n```javascript\nvar point = ee.Geometry.Point([-80.42, 37.22]);\nvar landsat = ee.ImageCollection(\"LANDSAT/LC08/C01/T1_TOA\")\nvar  image = ee.Image(landsat     \n                      .filterBounds(point)     \n                      .filterDate('2015-06-01', '2015-09-01')\n                      .sort('CLOUD_COVER')\n                      .first());\nvar trueColor = {bands: ['B4', 'B3', 'B2'], \n                 min: 0, max: 0.3};   \nMap.centerObject(point, 12);                  \nMap.addLayer(image, trueColor, 'image');  \n```\n\nNow that we have the true color baseline image, we can build the NDVI index and visualize it. For visualization, we are creating a custom palette, where low values trend towards white and high values trend towards green. \n\n```javascript\nvar point = ee.Geometry.Point([-80.42, 37.22]);\nvar landsat = ee.ImageCollection(\"LANDSAT/LC08/C01/T1_TOA\")\nvar image = ee.Image(landsat     \n                      .filterBounds(point)     \n                      .filterDate('2015-06-01', '2015-09-01')\n                      .sort('CLOUD_COVER')\n                      .first());\nvar  ndvi = image.normalizedDifference(['B5', 'B4']);  \nvar  vegPalette = ['white', 'green']; \nMap.centerObject(point, 12);\nMap.addLayer(ndvi, {min: -1, max: 1,  \n                    palette: vegPalette}, 'NDVI'); \n```\n\nUse the **Inspector** to check pixel values in areas of vegetation and non-vegetation. Also inspect how the values of lakes and rivers compare.\n\n![NDVI Image, Blacksburg, VA](im/im_03_02.png)\n\n> **Question 1A**: What are some of the sample pixel values of the NDVI in the below categories. Indicate which parts of the images you used and how you determined what each of their values were.\n>\n> 1. Vegetation\n> 2. Urban features\n> 3. Bare earth \n> 4. Water\n\n#### Enhanced Vegetation Index (EVI) \n\nThe Enhanced Vegetation Index (EVI) is designed to minimize saturation and background effects in NDVI ([Huete, 2002](http://www.sciencedirect.com/science/article/pii/S0034425702000962)). \n\n$$\\text{EVI} = 2.5 * (\\text{NIR} - \\text{red}) / (\\text{NIR} + 6 * \\text{red} - 7.5 * \\text{blue} + 1)$$\n\nSince it is not a normalized difference index, we need to build a unique [expression](https://developers.google.com/earth-engine/image_math#expressions) and then identify all of the different segments. Programmatically, bands are specifically referenced with the help of [an object](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Grammar_and_Types#Object_literals) that is passed as the second argument to `image.expression()` (everything within the curly brackets). \n\n```javascript\nvar point = ee.Geometry.Point([-80.42, 37.22]);\nvar landsat = ee.ImageCollection(\"LANDSAT/LC08/C01/T1_TOA\")\nvar  image = ee.Image(landsat     \n                      .filterBounds(point)     \n                      .filterDate('2015-06-01', '2015-09-01')\n                      .sort('CLOUD_COVER')\n                      .first());\n// Build the expression\nvar exp = '2.5  * ((NIR - RED) / (NIR + 6 * RED - 7.5 * BLUE + 1))';\nvar evi = image.expression( exp, \n                            {'NIR': image.select('B5'),\n                             'RED': image.select('B4'),\n                             'BLUE': image.select('B2')\n                            });  \nvar vegPalette = ['white', 'green']; \nMap.centerObject(point, 12);\nMap.addLayer(evi,  \n             {min: -1, max: 1,  palette: vegPalette}, \n             'EVI');  \n```\n\n> **Question 1B**: Compare EVI to NDVI across those same land use categories as in the previous question. What do you observe -- how are the images and values similar or different across the two indices?\n\n#### Normalized Difference Water Index (NDWI)\n\nThe Normalized Difference Water Index (NDWI) was developed by [Gao (1996)](http://www.sciencedirect.com/science/article/pii/S0034425796000673) as an index to identify the water content within vegetation. SWIR stands for short-wave infrared, which is the Landsat band 6.  This is not an exact implementation of NDWI, according to the [OLI spectral response](http://landsat.gsfc.nasa.gov/?p=5779), since OLI does not have a band in the right position (1.26 𝛍m) - but for our purposes, this is an approximation that does an acceptable job of identifying water content. \n\n$$\\text{NDWI} = (\\text{NIR} - \\text{SWIR})) / (\\text{NIR} + \\text{SWIR})$$\n\n```javascript\nvar point = ee.Geometry.Point([-80.42, 37.22]);\nvar landsat = ee.ImageCollection(\"LANDSAT/LC08/C01/T1_TOA\")\nvar  image = ee.Image(landsat     \n                      .filterBounds(point)     \n                      .filterDate('2015-06-01', '2015-09-01')\n                      .sort('CLOUD_COVER')\n                      .first());\nvar ndwi = image.normalizedDifference(['B5', 'B6']);  \nvar waterPalette = ['white', 'blue'];   \nMap.centerObject(point, 12);\nMap.addLayer(ndwi,  \n             {min: -1, max: 1,  palette: waterPalette}, \n             'NDWI');  \n```\n\n#### Normalized Difference Water *Body* Index (NDWBI)\n\nThe fact that two different NDWI indices were independently invented in 1996 complicates things. While the NDWI looks at water content within vegetation, the NDWBI is built to identify bodies of water (rivers, lakes, oceans). To distinguish, define the Normalized Difference Water *Body* Index (NDWBI) as the index described in [McFeeters (1996)](http://www.tandfonline.com/doi/abs/10.1080/01431169608948714#.VkThFHyrTlM):\n\n$$\\text{NDWBI} = (\\text{green} - \\text{NIR}) / (\\text{green} + \\text{NIR})$$\n\nAs previously, implement NDWBI with `normalizedDifference()` and display the result.\n\n```javascript\nvar point = ee.Geometry.Point([-80.42, 37.22]);\nvar landsat = ee.ImageCollection(\"LANDSAT/LC08/C01/T1_TOA\")\nvar  image = ee.Image(landsat     \n                      .filterBounds(point)     \n                      .filterDate('2015-06-01', '2015-09-01')\n                      .sort('CLOUD_COVER')\n                      .first());\nvar waterPalette = ['white', 'blue'];            \nvar ndwbi = image.normalizedDifference(['B3', 'B5']);   \nMap.addLayer(ndwbi, \n             {min: -1, \n              max: 0.5,  \n              palette: waterPalette}, \n             'NDWBI');   \n```\n\n![NDWBI Image](im/im_03_03.png)\n\nYou can combine the code blocks to compare the actual values at different pixel locations. Use inspector to test out different land areas.\n\n> **Question 2**: Compare NDWI and NDWBI. What do you observe? What do each of the indices try to focus on?\n\n#### Normalized Difference Bare Index (NDBI)\n\nThe Normalized Difference Bare Index (NDBI) was developed by [Zha, 2003)](http://www.tandfonline.com/doi/abs/10.1080/01431160304987) to aid in the differentiation of urban areas by using a combination of the shortwave and near infrared. \n\n$$\\text{NDBI} = (\\text{SWIR} - \\text{NIR}) / (\\text{SWIR} + \\text{NIR})$$\n\nNote that NDBI is the negative of NDWI. Compute NDBI and display with a suitable palette. (Check [this reference](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array) to demystify the palette reversal)\n\n```javascript\nvar point = ee.Geometry.Point([-80.42, 37.22]);\nvar landsat = ee.ImageCollection(\"LANDSAT/LC08/C01/T1_TOA\")\nvar  image = ee.Image(landsat     \n                      .filterBounds(point)     \n                      .filterDate('2015-06-01', '2015-09-01')\n                      .sort('CLOUD_COVER')\n                      .first());\nvar waterPalette = ['white', 'blue'];   \nvar barePalette =  waterPalette.slice().reverse(); \nvar ndbi = image.normalizedDifference(['B6', 'B5']);   \nMap.addLayer(ndbi, {min: -1, max: 0.5,  palette: barePalette}, 'NDBI');  \n```\n\n#### Burned Area Index (BAI) \n\nThe Burned Area Index (BAI) was developed by [Chuvieco et al. (2002)](http://www.tandfonline.com/doi/abs/10.1080/01431160210153129) to assist in the delineation of burn scars and assessment of burn severity. It is based on maximizing the spectral characteristics of charcoal reflectance. To examine burn indices, load an image from 2013 showing the [Rim fire](https://en.wikipedia.org/wiki/Rim_Fire) in the Sierra Nevadas. We'll start by creating a true image of the area to see how well this index highlights the presence of wildfire. \n\n```javascript\nvar point = ee.Geometry.Point(-120.083, 37.850);\nvar landsat = ee.ImageCollection(\"LANDSAT/LC08/C01/T1_TOA\")\nvar image = ee.Image(landsat\n  .filterBounds(point)\n  .filterDate('2013-08-17', '2013-09-27')\n  .sort('CLOUD_COVER')\n  .first());\nvar trueColor = {bands: ['B4', 'B3', 'B2'], \n                 min: 0, max: 0.3};   \nMap.centerObject(point, 12);                  \nMap.addLayer(image, trueColor, 'image');  \n```\n\nClosely examine the true color display of this image. Can you spot where the fire occurred? If difficult, let's look at the burn index.\n\n![True Color Fire Area](im/im_03_04.png)\n\n```javascript\nvar point = ee.Geometry.Point(-120.083, 37.850);\nvar landsat = ee.ImageCollection(\"LANDSAT/LC08/C01/T1_TOA\")\nvar image = ee.Image(landsat\n  .filterBounds(point)\n  .filterDate('2013-08-17', '2013-09-27')\n  .sort('CLOUD_COVER')\n  .first());\nvar trueColor = {bands: ['B4', 'B3', 'B2'], \n                 min: 0, max: 0.3};   \nMap.centerObject(point, 12);                  \nMap.addLayer(image, trueColor, 'image');  \n\n// Build Burn Index expression\nvar exp = '1.0  / ((0.1 - RED)**2 + (0.06 - NIR)**2)';\nvar bai = image.expression(exp,   \n                               {'NIR': image.select('B5'),   \n                                'RED': image.select('B4') });\n                                \nvar burnPalette = ['green', 'blue', 'yellow', 'red'];   \nMap.addLayer(bai, {min: 0, max: 400,  palette: burnPalette}, 'BAI');\n```\n\nThe charcoal burn area is now very evident. Being that Landsat has historical data and a wide array of sensors, this can be a powerful way to understand natural phenomena. \n\n![Burn Index](im/im_03_05.png)\n\n> **Question 3**: Compare NDBI and the BAI displayed results -- what do you observe?\n\n#### Normalized Burn Ratio Thermal (NBRT)\n\nThe Normalized Burn Ratio Thermal (NBRT) was developed based on the idea that burned land has low NIR reflectance (less vegetation), high SWIR reflectance (think ash), and high brightness temperature ([Holden et al. 2005](http://www.tandfonline.com/doi/abs/10.1080/01431160500239008)). Unlike the other indices, a lower NBRT means a higher likelihood of recent burn (for visualization, reverse the scale). This index can be used to diagnose the severity of wildfires (see [van Wagtendonk et al. 2004](http://www.sciencedirect.com/science/article/pii/S003442570400152X)).\n\n```javascript\nvar point = ee.Geometry.Point(-120.083, 37.850);\nvar landsat = ee.ImageCollection(\"LANDSAT/LC08/C01/T1_TOA\")\nvar image = ee.Image(landsat\n  .filterBounds(point)\n  .filterDate('2013-08-17', '2013-09-27')\n  .sort('CLOUD_COVER')\n  .first());\nvar trueColor = {bands: ['B4', 'B3', 'B2'], \n                 min: 0, max: 0.3};   \nMap.centerObject(point, 12);                  \nMap.addLayer(image, trueColor, 'image');  \n// Build Burn Index expression\nvar exp = '(NIR - 0.0001 * SWIR *  Temp) / (NIR + 0.0001 * SWIR * Temp)'\nvar nbrt = image.expression(exp,   \n                                {'NIR': image.select('B5'),   \n                                 'SWIR': image.select('B7'),   \n                                 'Temp': image.select('B11')  \n                                });  \nvar burnPalette = ['green', 'blue', 'yellow', 'red'];   \nMap.addLayer(nbrt, {min: 1, max: 0.9,  palette: burnPalette}, 'NBRT'); \n```\n\n#### Normalized Difference Snow Index (NDSI)\n\nThe Normalized Difference Snow Index (NDSI) was designed to estimate the amount of a pixel covered in snow ([Riggs et al. 1994](http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=399618&tag=1)).\n\n$$\\text{NDSI} = (\\text{green} - \\text{SWIR}) /(\\text{green} + \\text{SWIR})$$\n\nLet's look at Aspen, Colorado and use Landsat 8 data in the winter. You can use the layer manager to turn on and off the snow layer to compare results with the true color image. How does it compare? Reference the spectral reflectance chart at the beginning of the lab and look at the profile for snow. You will see that it has a distinct profile. In the image below, it does a very good job of matching the true color image - valleys and roads that do not have snow on them are accurately shown. \n\n```javascript\nvar point = ee.Geometry.Point([-106.81, 39.19]);\nvar landsat = ee.ImageCollection(\"LANDSAT/LC08/C01/T1_TOA\")\nvar image = ee.Image(landsat\n  .filterBounds(point)\n  .filterDate('2013-11-17', '2014-03-27')\n  .sort('CLOUD_COVER')\n  .first());\nvar trueColor = {bands: ['B4', 'B3', 'B2'], \n                 min: 0, max: 0.3};   \nMap.centerObject(point, 12);                  \nMap.addLayer(image, trueColor, 'image');  \nvar ndsi = image.normalizedDifference(['B3', 'B6']);      \nvar snowPalette = ['red', 'green', 'blue', 'white'];   \nMap.addLayer(ndsi,              \n             {min: -0.5, max: 0.5,  palette: snowPalette},              \n             'NDSI');  \n```\n\n![Snow Index](im/im_03_06.png)\n\n## Transformations\n\nWe've gone over indices to highlight unique characteristics in our imagery by utilizing the bands outside of the visible spectrum. \n\nLinear transforms are linear combinations of input pixel values. These can result from a variety of different strategies, but a common theme is that pixels are treated as arrays of band values, and we can use these arrays to create weighted values for specific purposes.\n\n#### Tasseled cap (TC)\n\nBased on observations of agricultural land covers in the NIR-red spectral space, [Kauth and Thomas (1976)](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.461.6381&rep=rep1&type=pdf) devised a [rotational transform](https://en.wikipedia.org/wiki/Change_of_basis) of the form \n\n$$p_1 = R^T p_0$$\n\n\nwhere $$p_0$$ is the original pixel vector (a stack of the *p* band values as an [Array](https://developers.google.com/earth-engine/arrays_intro)), **$$p_1$$** is the rotated pixel and **R** is an [orthonormal basis](https://en.wikipedia.org/wiki/Orthonormal_basis) of the new space (therefore is $$R^T$$ its inverse). Kauth and Thomas found **R** by defining the first axis of their transformed space to be parallel to the soil line in the following chart, then used the [Gram-Schmidt process](https://en.wikipedia.org/wiki/Gram–Schmidt_process) to find the other basis vectors.\n\n\n\n![Tassled Cap](im/im_03_07.png)\n\nAssuming that **R** is available, one way to implement this rotation in Earth Engine is with arrays. Specifically, make an array of TC coefficients. Since these coefficients are for the TM sensor, get a less cloudy Landsat 5 scene. To do the matrix multiplication, first convert the input image from a multi-band image to an array image in which each pixel position stores an array. Do the matrix multiplication, then convert back to a multi-band image.\n\n\n\n```javascript\nvar point = ee.Geometry.Point([-106.81, 39.19]);\nvar coefficients = ee.Array([    \n  [0.3037, 0.2793, 0.4743, 0.5585, 0.5082, 0.1863],    \n  [-0.2848, -0.2435, -0.5436, 0.7243, 0.0840, -0.1800],\n  [0.1509, 0.1973, 0.3279, 0.3406, -0.7112, -0.4572],\n  [-0.8242, 0.0849, 0.4392, -0.0580, 0.2012, -0.2768],\n  [-0.3280, 0.0549, 0.1075, 0.1855, -0.4357, 0.8085],\n  [0.1084, -0.9022, 0.4120, 0.0573, -0.0251, 0.0238]\n]);  \nvar landsat = ee.ImageCollection(\"LANDSAT/LT05/C01/T1_TOA\")\nvar image = ee.Image(landsat\n  .filterBounds(point)\n  .filterDate('2008-06-01', '2008-09-01')\n  .sort('CLOUD_COVER')\n  .first());\nvar bands = ['B1', 'B2', 'B3', 'B4', 'B5', 'B7'];\n// Make an Array Image,  with a 1-D Array per pixel.\nvar arrayImage1D =  image.select(bands).toArray();\n// Make an Array Image  with a 2-D Array per pixel, 6x1.\nvar arrayImage2D = arrayImage1D.toArray(1);  \nvar componentsImage = ee.Image(coefficients)\n\t\t\t\t.matrixMultiply(arrayImage2D)\n// Get rid of the extra  dimensions.\n\t\t\t\t.arrayProject([0])  \n// Get a multi-band image  with TC-named bands.  \n\t\t\t\t.arrayFlatten(\n          [['brightness', 'greenness', 'wetness', 'fourth', 'fifth', 'sixth']]\n        );       \nvar vizParams = {\n  bands: ['brightness', 'greenness', 'wetness'],\n  min: -0.1, max: [0.5,  0.1, 0.1]\n};\nMap.addLayer(componentsImage, vizParams, 'TC components');  \n```\n\n![Tasseled Cap Image](im/im_03_08.png)\n\n> **Question 3:** Upload the a tasseled cap image from a point near Blacksburg, VA and interpret the output. what are some of the values that you can extract when using **Inspector**? Are the results meaningful? \n\n#### Principal Component Analysis (PCA)\n\nLike the Tasseled Cap transform, the [PCA transform](https://en.wikipedia.org/wiki/Principal_component_analysis) is a rotational transform in which the new basis is orthonormal, but the axes are determined from statistics of the input image, rather than empirical data. Specifically, the new basis is the [eigenvector](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors) of the image's [variance-covariance matrix](https://en.wikipedia.org/wiki/Covariance_matrix). As a result, the principal components are uncorrelated. To demonstrate, use the Landsat 8 image converted to an array image. Use the `reduceRegion()` [method](https://developers.google.com/earth-engine/reducers_reduce_region) to compute statistics (band covariances) for the image.\n\nA [*reducer*](https://developers.google.com/earth-engine/reducers_intro) is an object that tells Earth Engine what statistic to compute. Note that the result of the reduction is an object with one property, an array, that stores the covariance matrix. The next step is to compute the eigenvectors and eigenvalues of that covariance matrix. Since the eigenvalues are appended to the eigenvectors, slice the two apart and discard the eigenvectors. Perform the matrix multiplication, as with the TC components. Finally, convert back to a multi-band image and display the first PC.\n\nUse the [layer manager](https://developers.google.com/earth-engine/playground#layer-manager) to stretch the result appropriately. What do you observe? Try displaying some of the other principal components. The image parameters in the code chunk below are built specifically for Principal Component 1. \n\n```javascript\nvar point = ee.Geometry.Point([-80.42, 37.22]);\nvar landsat = ee.ImageCollection(\"LANDSAT/LC08/C01/T1_TOA\")\nvar image = ee.Image(landsat\n  .filterBounds(point)\n  .filterDate('2013-11-17', '2014-03-27')\n  .sort('CLOUD_COVER')\n  .first());\nvar bands = ['B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B10', 'B11'];\nvar arrayImage =  image.select(bands).toArray();  \nvar covar = arrayImage.reduceRegion({\n  reducer: ee.Reducer.covariance(),\n  maxPixels: 1e9\n});\nvar covarArray = ee.Array(covar.get('array'));  \nvar eigens = covarArray.eigen();  \nvar eigenVectors = eigens.slice(1, 1);  \nvar principalComponents = \t\t\tee.Image(eigenVectors).matrixMultiply(arrayImage.toArray(1));  \nvar pcImage = principalComponents      \n\t\t\t\t.arrayProject([0])    \n// Make the one band  array image a multi-band image, [] -> image.    \n\t\t\t\t.arrayFlatten(\n          [['pc1', 'pc2', 'pc3', 'pc4', 'pc5', 'pc6', 'pc7', 'pc8']]\n        );      \n// Customize the visual parameters for PC1\nvar imageVisParam = {\n  \"opacity\":1,\n  \"bands\":[\"pc1\"],\n  \"min\":-420,\"max\":-400,\n  \"gamma\":1};\nMap.centerObject(point, 10);    \nMap.addLayer(pcImage.select('pc1'), imageVisParam, 'PC');\n```\n\n![PCA Image](im/im_03_09.png)\n\n> **Question 4**: How much did you need to stretch the results to display outputs for principal component 2? \n>\n> Display and upload images of each the other principal components, stretching each band as needed for visual interpretation and indicating how you selected the min and max values. \n>\n> How do you interpret each PC band? On what basis do you make that interpretation? \n\n#### Spectral Unmixing\n\nThe [linear spectral mixing model](http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=974727&tag=1) is based on the assumption that each pixel is a mixture of \"pure\" spectra. The pure spectra, called *endmembers*, are from land cover classes such as water, bare land, vegetation. The goal is to solve the following equation for **f**, the *P*x1 vector of endmember fractions in the pixel:  \n\n$$Sf = p$$\n\nwhere **S** is a *B*x*P* matrix in which the columns are *P* pure endmember spectra (known) and **p** is the *B*x1 pixel vector when there are *B* bands (known). In this example, $B= 6$: \n\nThe first step is to get the endmember spectra, which we can do by computing the mean spectra in polygons around regions of pure land cover. In this example, we will use a location in northern Washington State and use the geometry tools to select homogeneous areas of bare land, vegetation and water.\n\nUsing the [geometry drawing tools](https://developers.google.com/earth-engine/playground#geometry-tools), make three layers clicking **+ new layer**. In the first layer, digitize a polygon around pure bare land, in the second layer make a polygon of pure vegetation, and in the third layer, make a water polygon. Name the imports `bare`, `water` and, and `vegetation`, respectively. \n\nNote: For a starting point, we included some basic polygons but feel free to replace in a region of your choice. \n\n```javascript\nvar point = ee.Geometry.Point([-123.25, 48.11]);\nvar landsat = ee.ImageCollection(\"LANDSAT/LC08/C01/T1_TOA\")\nvar image = ee.Image(landsat\n  .filterBounds(point)\n  .filterDate('2013-06-17', '2014-03-27')\n  .sort('CLOUD_COVER')\n  .first());\n// Polygons of bare earth, water and vegetation\nvar bare = \n    ee.Geometry.Polygon(\n        [[[-123.2370707334838, 48.1151452657945],\n          [-123.2370707334838, 48.11351208612645],\n          [-123.23410957473136, 48.11351208612645],\n          [-123.23410957473136, 48.1151452657945]]], null, false);\nvar water = \n    ee.Geometry.Polygon(\n        [[[-123.2748188020549, 48.12059599002954],\n          [-123.2748188020549, 48.118074835535865],\n          [-123.2673086168132, 48.118074835535865],\n          [-123.2673086168132, 48.12059599002954]]], null, false);\nvar vegetation = \n    ee.Geometry.Polygon(\n        [[[-123.27462568300582, 48.11533866992809],\n          [-123.27462568300582, 48.114163936320416],\n          [-123.27215805071212, 48.114163936320416],\n          [-123.27215805071212, 48.11533866992809]]], null, false);\nvar unmixImage = image.select(['B2', 'B3', 'B4', 'B5', 'B6', 'B7']);\n```\n\nCheck the polygons you made by charting mean spectra in them using [Chart.image.regions()](https://developers.google.com/earth-engine/charts_image_regions):\n\nYour chart should look something like:\n\n![Charting Spectra](im/im_04_03.png)\n\n```javascript\nvar unmixImage = image.select(['B2', 'B3', 'B4', 'B5', 'B6', 'B7']);  \nprint(Chart.image.regions(unmixImage, ee.FeatureCollection([\n    ee.Feature(bare, {label: 'bare'}), \n    ee.Feature(water, {label: 'water'}),\n    ee.Feature(vegetation, {label: 'vegetation'})]), \n  ee.Reducer.mean(), 30, 'label', [0.48, 0.56, 0.65, 0.86, 1.61, 2.2]));\n```\n\nUse the [reduceRegion() method](https://developers.google.com/earth-engine/reducers_reduce_region) to compute mean spectra in the polygons you made. Note that the return value of reduceRegion() is a Dictionary, with reducer output keyed by band name. Get the means as a list by calling `values()`. Each of these three lists represents a mean spectrum vector. Stack the vectors into a 6x3 Array of endmembers by concatenating them along the 1-axis (or columns direction).\n\nTurn the 6-band input image into an image in which each pixel is a 1D vector (`toArray()`), then into an image in which each pixel is a 6x1 matrix (`toArray(1)`). Now that the dimensions match, in each pixel, solve the equation for **f**. Finally, convert the result from a 2D array image into a 1D array image (`arrayProject()`), then to a multi-band image (`arrayFlatten()`). The three bands correspond to the estimates of bare, vegetation and water fractions in **f**. Display the result where bare is red, vegetation is green, and water is blue (the `addLayer()` call expects bands in order, RGB)\n\n```javascript\nvar bareMean = unmixImage.reduceRegion(\n  ee.Reducer.mean(), bare, 30).values();   \nvar waterMean = unmixImage.reduceRegion(\n  ee.Reducer.mean(), water, 30).values();   \nvar vegMean = unmixImage.reduceRegion(\n  ee.Reducer.mean(), vegetation, 30).values();  \nvar endmembers = ee.Array.cat([bareMean,  vegMean, waterMean], 1);  \nvar arrayImage = unmixImage.toArray().toArray(1);\nvar unmixed =  ee.Image(endmembers).matrixSolve(arrayImage);\nvar unmixedImage = unmixed.arrayProject([0])\n\t\t\t\t.arrayFlatten(\n          [['bare', 'veg', 'water']]\n        );  \nMap.addLayer(unmixedImage, {}, 'Unmixed');  \n```\n\n![Spectral Unmixing Image](im/im_03_10.png)\n\n> Question 5: Repeat this process for Blacksburg, VA. Upload the mean spectra chart you generated for bare, water, and land and the map. Interpret the output of the image by selecting different pixels with **Inspector**\n\n#### Hue-Saturation-Value Transform\n\nThe Hue-Saturation-Value (HSV) model [is a color transform of the RGB color space](https://en.wikipedia.org/wiki/HSL_and_HSV). Among many other things, it is useful for [pan-sharpening](https://en.wikipedia.org/wiki/Pansharpened_image). This involves converting an RGB to HSV, swapping the panchromatic band for the value (V), then converting back to RGB. For example, using the Landsat 8 scene:\n\n```javascript\nvar point = ee.Geometry.Point([-80.42, 37.22]);\nvar landsat = ee.ImageCollection(\"LANDSAT/LC08/C01/T1_TOA\")\nvar image = ee.Image(landsat\n  .filterBounds(point)\n  .filterDate('2013-06-17', '2014-03-27')\n  .sort('CLOUD_COVER')\n  .first());\n// Convert Landsat RGB bands to HSV   \nvar hsv = image.select(['B4', 'B3', 'B2']).rgbToHsv();\n// Convert back to RGB,  swapping the image panchromatic band for the value.\nvar rgb = ee.Image.cat([\n  hsv.select('hue'),\n  hsv.select('saturation'),\n  image.select(['B8'])]).hsvToRgb();\nMap.centerObject(point, 12);\nMap.addLayer(rgb, {max: 0.4}, 'Pan-sharpened');  \n```\n\n> **Question 6**: Compare the pan-sharpened image with the original image. What do you notice that's different? The same? \n\n## Spectral Transformation\n\n### Linear Filtering\n\nIn the present context, linear *filtering* (or [convolution](http://www.dspguide.com/ch24/1.htm)) refers to a linear combination of pixel values in a 'neighborhood', or [kernel](https://en.wikipedia.org/wiki/Kernel_(image_processing)), where the weights of the kernel determine the coefficients in the linear combination (for this lab, the terms *kernel* and *filter* are interchangeable.) Filtering an image can be useful for extracting image information at different [spatial frequencies](http://www.dspguide.com/ch24/5.htm) by reducing noise. For this reason, smoothing filters are called *low-pass* filters (they let *low*-frequency data *pass* through) and edge detection filters are called *high-pass* filters. To implement filtering in Earth Engine use [image.convolve()](https://developers.google.com/earth-engine/guides/image_convolutions) with an ee.Kernel for the argument.\n\n#### Smoothing\n\nSmoothing means to convolve an image with a smoothing kernel. \n\nA simple smoothing filter is a square kernel with uniform weights that sum to one. Convolving with this kernel sets each pixel to the mean of its neighborhood. Print a square kernel with uniform weights (this is sometimes called a \"pillbox\" or \"boxcar\" filter):\n\nExpand the kernel object in the console to see the weights. This kernel is defined by how many pixels it covers (i.e. `radius` is in units of 'pixels'). A kernel with radius defined in 'meters' adjusts its size in pixels, so you can't visualize its weights, but it's more flexible in terms of adapting to inputs of different scale. In the following, use kernels with radius defined in meters except to visualize the weights.\n\n```javascript\nvar point = ee.Geometry.Point([-80.42, 37.22]);\nvar naip = ee.ImageCollection(\"USDA/NAIP/DOQQ\")\nvar image = ee.Image(naip\n  .filterBounds(point)\n  .filterDate('2013-06-17', '2017-03-27')\n  .first());\n// Print a uniform kernel to see its weights.\nprint('A uniform kernel:', ee.Kernel.square(2));\n\n```\n\n![Kernel Description](im/im_03_11.png)\n\nDefine a kernel with 2-meter radius (which corresponds to how many pixels in the NAIP image? Hint: try [projection.nominalScale()](https://developers.google.com/earth-engine/guides/projections)), convolve the image with the kernel and compare the input image with the smoothed image:\n\n```javascript\n// Define a square, uniform kernel.\nvar uniformKernel = ee.Kernel.square({\n radius: 2,\n units: 'meters',\n});\n// Filter the image by convolving with the smoothing filter.\nvar smoothed = image.convolve(uniformKernel);\nvar trueColorVis = {\n  min: 0.0,\n  max: 255.0,\n};\nMap.centerObject(point, 12);\nMap.addLayer(smoothed, trueColorVis, 'smoothed image');\n```\n\nTo make the image even smoother, try increasing the size of the neighborhood by increasing the pixel radius. to the human eye, the image is blurrier, but in many Machine Learning and Computer Vision algorithms, this process improves our output by reducing noise. \n\n\nA Gaussian kernel can also be used for smoothing. Think of filtering with a Gaussian kernel as computing the weighted average in each pixel's neighborhood. \n\n```javascript\n// Print a Gaussian kernel to see its weights.\nprint('A Gaussian kernel:', ee.Kernel.gaussian(2));\n// Define a square Gaussian kernel:\nvar gaussianKernel = ee.Kernel.gaussian({\n radius: 2,\n units: 'meters',\n});\n// Filter the image by convolving with the Gaussian filter.\nvar gaussian = image.convolve(gaussianKernel);\nvar trueColorVis = {\n  min: 0.0,\n  max: 255.0,\n};\nMap.centerObject(point, 12);\nMap.addLayer(gaussian, trueColorVis, 'smoothed image');\n```\n\n> **Question 7**: What happens as you increase the pixel radius for each smoothing? What differences can you discern between the weights and the visualizations of the two smoothing kernels?\n\n#### Edge Detection\n\nConvolving with an edge-detection kernel is used to find rapid changes in values that usually signify the edges of objects in the image data. \n\nA classic edge detection kernel is the [Laplacian](https://en.wikipedia.org/wiki/Discrete_Laplace_operator) kernel. Investigate the kernel weights and the image that results from convolving with the Laplacian. Other edge detection kernels include the [Sobel](https://en.wikipedia.org/wiki/Sobel_operator), [Prewitt](https://en.wikipedia.org/wiki/Prewitt_operator) and [Roberts](https://en.wikipedia.org/wiki/Roberts_cross) kernels. [Learn more about additional edge detection methods in Earth Engine](https://developers.google.com/earth-engine/image_edges).    \n\n```javascript\nvar point = ee.Geometry.Point([-80.42, 37.22]);\nvar naip = ee.ImageCollection(\"USDA/NAIP/DOQQ\")\nvar image = ee.Image(naip\n  .filterBounds(point)\n  .filterDate('2013-06-17', '2017-03-27')\n  .first());\n// Define a Laplacian, or edge-detection kernel.\nvar laplace = ee.Kernel.laplacian8({ normalize: false });\n// Apply the edge-detection kernel.\nvar edges = image.convolve(laplace);\nvar trueColorVis = {\n  min: 0.0,\n  max: 255.0,\n  format: 'png'\n};\nMap.centerObject(point, 12);\nMap.addLayer(edges, trueColorVis,'edges');\n```\n\n![Edge Detection](im/im_03_12.png)\n\n> **Question 8:** Choose another edge detection method and upload the image - what characteristics do you see, and how does it compare to Laplacian Edge detection? \n\n## Additional Exercises\n\n> **Question 9**: Look in google scholar to identify 2-3 publications that have used NDVI and two-three that used EVI. For what purposes were these indices used and what was the justification provided for that index? \n\n> **Question 10**: Discuss a spectral index that we did not cover in this lab relates to your area of research/interest. What is the the name of the spectral index, the formula used to calculate it, and what is it used to detect? Provide a citation of an academic article that has fruitfully used that index. \n\n> **Question 11**: Find 1-2 articles that use any of the linear transformation methods we practiced in this lab in the service of addressing an important social issue (e.g., one related to agriculture, environment, or development). Provide the citations and discussed how the transformation is used and how it's justified in the article. ",
   "metadata": {
    "cell_id": "4405daf12b9b4932bc47baec075a7805",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 34867.8125
   }
  },
  {
   "cell_type": "markdown",
   "source": "",
   "metadata": {
    "cell_id": "4312be08480d4feda3fe5e3a8a1dc948",
    "tags": [],
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=b8aff9fa-104a-41a3-b121-f629ebd12bab' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_notebook_id": "edacfa2e-10e3-4b48-aa79-3b55c12175eb",
  "deepnote_execution_queue": []
 }
}