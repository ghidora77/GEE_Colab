{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Lab 02 - Digital Imagery & Image Processing\n\n\n## Overview\n\nIn this lab, we will search for and visualize imagery in Google Earth Engine. We will discuss the difference between radiance and reflectance, make true color and false color composites from different bands and visually identify land cover types based on characteristics from the imagery. We will also discuss atmospheric effect on data collection by looking at the different data products available. \n\n####  Learning Outcomes\n\n- Extract single scenes from collections of images\n- Create and visualize different composites according\n- Use the Inspector tab to assess pixel values\n- Understand the difference between radiance and reflectance through visualization\n\n## Searching for Imagery\n\nThe Landsat program is a joint program between NASA and the United States Geological Survey (USGS) that has launched a sequence of Earth observation satellites (Landsat 1-9).  Originating in 1984, the Landsat program provides the [longest continuous observation of the Earth's surface](https://www.youtube.com/embed/ZZx1xmNGcXI?list=PLD240BBC85537B9BE). Take the time to monitor some of the fascinating [timelapses](https://earthengine.google.com/timelapse/) using Landsat to showcase things like urban development, glacial retreat and deforestation.  \n\nLet's load a Landsat scene over our region of interest, inspect the units and plot the radiance. Specifically, use imagery from the Landsat 8, the most recent of the [sequence of Landsat satellites](https://www.usgs.gov/core-science-systems/nli/landsat/landsat-8) (at the time of writing, Landsat 9 just launched and data is not yet available). \n\nTo inspect a Landsat 8 image (also called a *scene*) in our region of interest (ROI), we can choose a point to center our map, filter the image collection to get a scene with few clouds, and display information about the image in the console.\n\nYou can either scroll to the area on the map you're interested in and choose a point or use the search bar to find your location. Use the geometry tool to make a point in Blacksburg, VA (for these exercises we will include the point location in the script). \n\n![Using Search Bar](im/im_02_01a.png)\n\nWe will specifically be using USGS Landsat 8 Collection 1 Tier 1 Raw Scenes - if you read the documentation, the values refer to scaled, calibrated at-sensor radiance. Tier 1 means it is ready for analysis and is the highest quality imagery. There's quite a bit to learn about how the Landsat data is processed - if you will be working with Landsat extensively, take the time to read the Data Users [Handbook](https://www.usgs.gov/landsat-missions/landsat-8-data-users-handbook) for more information.\n\nWe will filter the `ImageCollection` by date (year 2014) and location (to the ROI, which for this exercise is Blacksburg, VA), sort by a metadata property included in the imagery called `CLOUD_COVER` and get the first image out of this sorted collection.\n\n```javascript\nvar point = ee.Geometry.Point([-80.42, 37.22]);\nvar landsat = ee.ImageCollection(\"LANDSAT/LC08/C01/T1\")\n//  Note that we need to cast the result of first() to Image.   \nvar image = ee.Image(landsat        \n                     //  Filter to get only images in the specified range.  \n                     .filterDate('2014-01-01',  '2014-12-31')        \n                     //  Filter to get only images at the location of the point.     \n                     .filterBounds(point)        \n                     //  Sort the collection by a metadata property.     \n                     .sort('CLOUD_COVER')        \n                     //  Get the first image out of this collection.     \n                     .first());  \n//  Print the information to the console \nprint('A Landsat scene:', image);  \n```\n\nThe variable `image` now stores a reference to an object of type `ee.Image`. In other words, we have taken the image collection and reduced it down to a single image, which is now ready for visualization. \n\nBefore we visualize the data, go to the console and click on the dropdown. \n\n![Image Properties](im/im_02_01b.png)\n\n\n\nExpand and explore the image by clicking the triangle next to the image name to see more information stored in that object. Specifically, expand `properties` and inspect the long list of metadata items stored as properties of the image. This is where the `CLOUD_COVER` property you just used is stored.\n\nThere are band specific coefficients (`RADIANCE_ADD_*`, `RADIANCE_MULT_*` where \\* is a band name) in the metadata for converting from the digital number (DN) stored by the image into physical units of radiance. These coefficients will be useful in later exercises.\n\n## Visualizing Landsat Imagery\n\nRecall from the last lab that Landsat 8 measures radiance in multiple spectral bands. A common way to visualize images is to set the red band to display in red, the green band to display in green and the blue band to display in blue - just as you would create a normal photograph. This means trying to match the [spectral response of the instrument](http://landsat.gsfc.nasa.gov/?p=5779) to the spectral response of the photoreceptors in the human eye. It's not a perfect match but this is called a *true-color* image. When the display bands don't match human visual perception (as we will see later), the visualization is called a *false-color composite*. \n\n#### True Color Composite\n\nTo build a true color image we are building a variable called `trueColor`  that selects the red / green / blue bands in order and includes the min and max value to account for the appropriate radiometric resolution - this piece can be tricky, as it is unique for each dataset you work with. You can find the band names and min-max values to use from the dataset documentation page, but a great starting point is to use the 'code example' snippet for each dataset, which will set up the visualization parameters for you.  \n\n```javascript\nvar point = ee.Geometry.Point([-80.42, 37.22]);\nvar landsat = ee.ImageCollection(\"LANDSAT/LC08/C01/T1\")\n//  Note that we need to cast the result of first() to Image.   \nvar image = ee.Image(landsat        \n                     //  Filter to get only images in the specified range.  \n                     .filterDate('2014-01-01',  '2014-12-31')        \n                     //  Filter to get only images at the location of the point.     \n                     .filterBounds(point)        \n                     //  Sort the collection by a metadata property.     \n                     .sort('CLOUD_COVER')        \n                     //  Get the first image out of this collection.     \n                     .first());  \n//  Define visualization parameters in a JavaScript dictionary.   \nvar trueColor = {    \n  bands: ['B4', 'B3', 'B2'],    \n  min: 4000,    \n  max: 13000\n};  \n// Add the image to the  map, using the visualization parameters.   \nMap.addLayer(image, trueColor, 'true-color image');  \n```\n\nThere is more than one way to discover the appropriate min and max values to display. Try going to the **Inspector** tab and clicking somewhere on the map. The value in each band, in the pixel where you clicked, is displayed as a list in the console. Try clicking on dark and bright objects to get a sense of the range of pixel values. Also, [layer manager](https://developers.google.com/earth-engine/playground#layer-manager) in the upper right of the map display lets you automatically compute a linear stretch based on the pixels in the map display. \n\n#### False Color Composite\n\nLet's do the same thing, but this time we will build a false-color composite. This particular set of bands results in a *color-IR composite* because the near infra-red (NIR) band is set to red. As you inspect the map, look at the pixel values and try to find relationships between the NIR band and different land types. Using false color composites is a very common and powerful method of identifying land characteristics by leveraging the power of signals outside of the visible realm. Mining engineers commonly use hyperspectral data to pinpoint composites with unique signatures, and urban growth researchers commonly use the infrared band to pinpoint roads and urban areas. \n\n```javascript\nvar point = ee.Geometry.Point([-80.42, 37.22]);\nvar landsat = ee.ImageCollection(\"LANDSAT/LC08/C01/T1\")\n//  Note that we need to cast the result of first() to Image.   \nvar image = ee.Image(landsat        \n                     //  Filter to get only images in the specified range.  \n                     .filterDate('2014-01-01',  '2014-12-31')        \n                     //  Filter to get only images at the location of the point.     \n                     .filterBounds(point)        \n                     //  Sort the collection by a metadata property.     \n                     .sort('CLOUD_COVER')        \n                     //  Get the first image out of this collection.     \n                     .first());  \n//  Print the information to the console \nprint('A Landsat scene:', image);  \n//  Define visualization parameters in a JavaScript dictionary.   \n//  Define false-color visualization parameters.   \nvar falseColor = {\n  bands: ['B5', 'B4', 'B3'],    \n  min: 4000,    \n  max: 13000   \n};  \n// Add the image to the  map, using the visualization parameters.   \nMap.addLayer(image, falseColor, 'false-color composite'); \n```\n\n![False Color Imagery](im/im_02_02.png)\n\nRead through the Landsat data documentation and try playing with different band combinations, min and max values to build different visualizations. \n\n**Unique Feature**: You can include multiple visualization parameters in your script and toggle the layers on and off with the layer manager for easy comparison. \n\n![Layer Manager](im/im_02_03.png)\n\n\n\n## At-Sensor Radiance\n\nThe image data you have used so far is stored as a digital number that measures the intensity within the bit range - if data is collected in an 8-bit system, 255 would be very high intensity and 0 will be no intensity. To convert each digital number into a physical unit (at-sensor [radiance](https://en.wikipedia.org/wiki/Radiance) in Watts/m2/sr/𝝁m), we can use a linear equation:\n\n$$\nL_{\\lambda} = a_{\\lambda} * DN_{\\lambda} + b_{\\lambda}  \\qquad\n$$\n\n\nNote that every term is indexed by lamda ($\\lambda$, the symbol for wavelength) because the coefficients are different in each band. See [Chander et al. (2009)](http://www.sciencedirect.com/science/article/pii/S0034425709000169) for details on this linear transformation between DN and radiance. In this exercise, you will generate a radiance image and examine the differences in radiance from different targets.\n\nEarth Engine provides built-in functions for converting Landsat imagery to radiance in Watts/m2/sr/𝝁m. It will automatically reference the metadata values for each band and apply the equation for you, saving you the trouble of conducing numerous calculations.\n\nThis code applies the transformation to a subset of bands (specified by a list of band names) obtained from the image using select(). That is to facilitate interpretation of the radiance spectrum by removing the panchromatic band ('B8'), an atmospheric absorption band ('B9') and the QA band ('BQA'). \n\nNote that the visualization parameters are different to account for the radiance units.\n\n```javascript\nvar point = ee.Geometry.Point([-80.42, 37.22]);\nvar landsat = ee.ImageCollection(\"LANDSAT/LC08/C01/T1\")\n//  Note that we need to cast the result of first() to Image.   \nvar image = ee.Image(landsat        \n                     //  Filter to get only images in the specified range.  \n                     .filterDate('2014-01-01',  '2014-12-31')        \n                     //  Filter to get only images at the location of the point.     \n                     .filterBounds(point)        \n                     //  Sort the collection by a metadata property.     \n                     .sort('CLOUD_COVER')        \n                     //  Get the first image out of this collection.     \n                     .first());  \n//  Use these bands.    \nvar bands = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B10', 'B11'];  \n// Get an image that  contains only the bands of interest.   \nvar dnImage = image.select(bands);  \n// Apply the  transformation.   \nvar radiance =  ee.Algorithms.Landsat.calibratedRadiance(dnImage);  \n// Display the result.   \nvar radParams = {bands: ['B4', 'B3', 'B2'], min: 0, max: 100};  \nMap.addLayer(radiance, radParams, 'radiance');  \n```\n\nExamine the radiance image by using **Inspector** and clicking different land cover types on the map near Blacksburg, VA. Click the chart icon (![Chart](./im/im_02_04.png)) in the console to get a bar chart of the different radiance values for each pixel. If the shape of the chart resembles Figure 1, that's because the radiance (in bands 1-7) is mostly reflected solar irradiance. The radiance detected in bands 10-11 is thermal, and is *emitted* (not reflected) from the surface.\n\n![Electromagnetic Spectrum](im/im_02_05.png)\n\n## Top-of-Atmosphere (TOA) Reflectance \n\nThe Landsat sensor is in orbit approximately 700 kilometers above Earth. If we are focused on the imagery of remote sensing (as opposed to studying something like atmospheric conditions or ambient temperature), then we want to find insights about the surface of the earth. To understand the way we calculate information, there are three main components.\n\nDigital Number (DN) is a value that is associated with each pixel - it is generic (in that it is an intensity value dependent upon the bit range), and it allows you to visualize the image where all pixels are in context. In most cases, DN is appropriate for analysis, image processing, machine learning, etc.\n\nRadiance is the radiation that collected by a sensor - this includes radiation from the surface of Earth, radiation scattered by clouds, position of the sun relative to the Earth and sensor, etc. In general, we want to correct radiance values and convert to reflectance. \n\nReflectance is the ratio (unitless)  of energy from the sun to the energy reflected off Earth's surface. In fact, it's more complicated than this because radiance is a directional quantity, but this definition captures the basic idea  We can identify materials based on their reflectance spectra. Because this ratio is computed using whatever radiance the sensor measures (which may contain all sorts of atmospheric effects), it's called *at-sensor* or *top-of-atmosphere* (TOA) reflectance. \n\nTop of Atmosphere reflectance is the reflectance that includes the radiation from earth's surface and radiation from earth's atmosphere. \n\nLet's examine the spectra for TOA Landsat data. To get TOA data for Landsat, we can do the transformation using the built-in functions created by Earth Engine. We will be using 'USGS Landsat 8 Collection 1 Tier 1 TOA Reflectance' ImageCollection.\n\n```javascript\nvar point = ee.Geometry.Point([-80.42, 37.22]);\nvar landsat = ee.ImageCollection(\"LANDSAT/LC08/C01/T1_TOA\")\n//  Note that we need to cast the result of first() to Image.   \nvar image = ee.Image(landsat        \n                     //  Filter to get only images in the specified range.  \n                     .filterDate('2014-01-01',  '2014-12-31')        \n                     //  Filter to get only images at the location of the point.     \n                     .filterBounds(point)        \n                     //  Sort the collection by a metadata property.     \n                     .sort('CLOUD_COVER')        \n                     //  Get the first image out of this collection.     \n                     .first());  \nvar point = ee.Geometry.Point([-80.42, 37.22]);\n//  Use these bands.    \nvar bands = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B10', 'B11'];  \nMap.addLayer(image, \n             {bands: ['B4', 'B3', 'B2'], \n              min: 0, max: 0.3}, 'toa');  \n// Define reflective  bands as bands B1-B7. See the docs for slice().   \nvar reflectiveBands = bands.slice(0, 7);      \n// See  http://landsat.usgs.gov/band_designations_landsat_satellites.php   \nvar wavelengths = [0.44, 0.48, 0.56, 0.65, 0.86, 1.61, 2.2];      \n// Select only the  reflectance bands of interest.   \nvar reflectanceImage =  image.select(reflectiveBands);      \n// Define an object of  customization parameters for the chart.   \nvar options = {\n  title: 'Landsat  8 TOA spectrum in Blacksburg, VA',    \n               hAxis: {title: 'Wavelength  (micrometers)'},\n               vAxis: {title: 'Reflectance'},\n               lineWidth: 1,\n               pointSize: 4};      \n// Make the chart, using  a 30 meter pixel.   \nvar chart = ui.Chart.image.regions(\n  reflectanceImage, \n  point, null, 30, null, wavelengths)\n\t\t.setOptions(options);      \n// Display the chart.   \nprint(chart); \n```\n\n\nSince reflectance is a unitless ratio in [0, 1], change the visualization parameters to correctly display the TOA data:\n\nUsing **Inspector**, click several locations on the map and examine the resultant spectra. It should be apparent, especially if you chart the spectra, that the scale of pixel values in different bands is drastically different. Specifically, bands 10-11 are not in [0, 1].  The reason is that these are thermal bands, and are converted to brightness temperature, in Kelvin, as part of the TOA conversion. Very little radiance is reflected in this wavelength range; most is emitted from the Earth's surface. That emitted radiance can be used to estimate [brightness temperature](https://en.wikipedia.org/wiki/Brightness_temperature) using the inverted [Planck equation](https://en.wikipedia.org/wiki/Planck's_law). Examine the temperature of various locations. Now add this command to the TOA image before adding it to the map to get only bands 1-9 \n\n* ` .select('B([0-9])')`\n\nTo make plots of reflectance, select the reflective bands from the TOA image and use the Earth Engine [charting API](https://developers.google.com/earth-engine/charts). \n\nThere are several new methods in this code. The `slice()` method gets entries in a list based on starting and ending indices. Search the docs (on the **Docs** tab) for 'slice' to find other places this method can be used. Construction of the chart is handled by an object of customization parameters ([learn more about customizing charts](https://developers.google.com/earth-engine/charts_image_histogram)) passed to [Chart.image.regions()](https://developers.google.com/earth-engine/charts_image_regions). Customizing charts within GEE can be difficult, so spend time modifying the characteristics. \n\n> **Question 1**: Upload the TOA reflectance plot you generated for Blacksburg, VA and briefly describe the relationship of reflectance peaks and troughs in the chart to the electromagnetic spectrum. \n\n## Surface Reflectance \n\nThe ratio of upward radiance *at the Earth's surface* to downward radiance *at the Earth's surface* is called surface reflectance. Unlike TOA reflectance, in which this information is collected at the sensor, the radiances at the Earth's surface have been affected by the atmosphere. both the inbound and outbound radiance from the sun is affected by its path through the atmosphere to the sensor. Unravelling those effects is called atmospheric correction (\"compensation\" is probably a more accurate term) and is beyond our scope of this lab. However, most satellite imagery providers complete this correction for the consumers. While you could use the raw scenes directly, if your goal is conduct analysis quickly and effectively, using the corrected Surface Reflectance image collections are quite beneficial and will save you quite a bit of time.\n\nOn your own: To explore Landsat Surface Reflectance data, search 'Landsat 8 Surface Reflectance' and import the 'USGS Landsat 8 Surface Reflectance Tier 2' `ImageCollection`.  Filter to the same date, location and cloudiness as with the raw and TOA collections and get the first image.\n\n> **Question 2**: Upload the surface reflectance plot you just generated and briefly describe its features. What differs or remains the same between the TOA plot and the surface reflectance plot?\n\n> **Question 3**: When you add `sr` to the map, you will need to scale the imagery or change the visualization parameters. Why? Read the dataset description to find out.  \n>\n> Hint: What is the scale factor for bands 1-9?\n\n## Additional Exercises\n\n> **Question 4**: In your code, set the value of a variable called `azimuth` to the solar azimuth of the image from 1d. Do not hardcode the number. Use `get()`. Print the result and show you set the value of `azimuth`.\n\n> **Question 5**: Add a layer to the map in which the image from 1d is displayed with band 7 set to red, band 5 set to green and band 3 set to blue. Upload a visual of the layer and show how you would display the layer name as `falsecolor`. \n\n> **Question 6**: What is the brightness temperature of the given Blacksburg, VA point? \n>\n> Show how you make a variable in your code called temperature and set it to the band 10 brightness temperature. Use [this guide](https://developers.google.com/earth-engine/reducers_reduce_region) for help.\n\n```javascript\nvar point = ee.Geometry.Point([-80.42, 37.22]);\nvar  temperature = toaImage.reduceRegion(\n  { <YOUR SOLUTION HERE>   })\n\t\t.get(  <YOUR SOLUTION  HERE>);  \n```\n\n> **Question 7**: What is the surface reflectance (in [0,1], meaning you will need to apply the scale factor) in band 5 (NIR) at the Blacksburg, VA point? \n>\n> Show how you make a variable in your code called `reflectance` that stores this value.",
   "metadata": {
    "cell_id": "19c75728e8474a129d3d828a2de35375",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 9475.484375
   }
  },
  {
   "cell_type": "markdown",
   "source": "",
   "metadata": {
    "cell_id": "6286adfc1f844f8180e20b25bfc79374",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 46
   }
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=b8aff9fa-104a-41a3-b121-f629ebd12bab' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_notebook_id": "fe256bdb-ee82-4e1a-af04-2cce3be633f7",
  "deepnote_execution_queue": []
 }
}